{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.virtualenvs/humemai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "DEBUG:docker.utils.config:Trying paths: ['/home/tk/.docker/config.json', '/home/tk/.dockercfg']\n",
      "DEBUG:docker.utils.config:No config file found\n",
      "DEBUG:docker.utils.config:Trying paths: ['/home/tk/.docker/config.json', '/home/tk/.dockercfg']\n",
      "DEBUG:docker.utils.config:No config file found\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/11\" 200 834\n",
      "DEBUG:docker.utils.config:Trying paths: ['/home/tk/.docker/config.json', '/home/tk/.dockercfg']\n",
      "DEBUG:docker.utils.config:No config file found\n",
      "DEBUG:docker.utils.config:Trying paths: ['/home/tk/.docker/config.json', '/home/tk/.dockercfg']\n",
      "DEBUG:docker.utils.config:No config file found\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /version HTTP/11\" 200 834\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.47/networks/janusgraph-net HTTP/11\" 200 939\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.47/containers/cassandra/json HTTP/11\" 200 None\n",
      "DEBUG:humemai.janusgraph.utils.docker:cassandra container is already running.\n",
      "DEBUG:urllib3.connectionpool:http://localhost:None \"GET /v1.47/containers/janusgraph/json HTTP/11\" 200 None\n",
      "DEBUG:humemai.janusgraph.utils.docker:janusgraph container is already running.\n",
      "DEBUG:humemai.janusgraph.utils.docker:Waiting 10 seconds for the containers to warm up...\n",
      "INFO:gremlinpython:Creating DriverRemoteConnection with url 'ws://localhost:8182/gremlin'\n",
      "INFO:gremlinpython:Creating Client with url 'ws://localhost:8182/gremlin'\n",
      "INFO:gremlinpython:Creating GraphTraversalSource.\n",
      "INFO:gremlinpython:Creating GraphTraversalSource.\n",
      "DEBUG:humemai.janusgraph.humemai:Successfully connected to the Gremlin server.\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:bitsandbytes.cextension:Loading bitsandbytes native library from: /home/tk/.virtualenvs/humemai/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda124.so\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /meta-llama/Llama-3.2-3B-Instruct/resolve/main/generation_config.json HTTP/11\" 200 0\n",
      "DEBUG:gremlinpython:submit with bytecode '[['V'], ['drop'], ['none']]'\n",
      "DEBUG:gremlinpython:message '[['V'], ['drop'], ['none']]'\n",
      "DEBUG:gremlinpython:processor='traversal', op='bytecode', args='{'gremlin': [['V'], ['drop'], ['none']], 'aliases': {'g': 'g'}}'\n",
      "DEBUG:asyncio:Using selector: EpollSelector\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from humemai.janusgraph import Humemai\n",
    "from humemai.utils import disable_logger, parse_file_by_paragraph\n",
    "from humemai.janusgraph.agent import PromptAgent\n",
    "from pprint import pprint\n",
    "\n",
    "agent = PromptAgent(\n",
    "    num_hops_for_working_memory=4,\n",
    "    turn_on_logger=False,\n",
    "    llm_config={\n",
    "        \"model\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"quantization\": \"8bit\",\n",
    "        \"max_new_tokens\": 1024,\n",
    "    },\n",
    "    text2graph_template=\"text2graph_without_properties\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = parse_file_by_paragraph(\n",
    "    \"harry-potter-Sorcerer-chapter-one.txt\", least_newlines=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/111 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/tk/.virtualenvs/humemai/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 1/111 [00:11<21:52, 11.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 2/111 [00:20<17:49,  9.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 3/111 [00:39<25:04, 13.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  4%|▎         | 4/111 [01:04<33:13, 18.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▍         | 5/111 [01:30<37:01, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  5%|▌         | 6/111 [02:00<42:18, 24.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  6%|▋         | 7/111 [02:42<52:13, 30.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 8/111 [03:28<1:00:14, 35.09s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 9/111 [04:32<1:14:59, 44.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 10/111 [05:35<1:24:04, 49.95s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|▉         | 11/111 [06:41<1:31:37, 54.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 12/111 [07:46<1:35:38, 57.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 12%|█▏        | 13/111 [08:51<1:38:03, 60.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 13%|█▎        | 14/111 [09:14<1:18:45, 48.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▎        | 15/111 [10:19<1:26:09, 53.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 14%|█▍        | 16/111 [10:41<1:09:44, 44.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▌        | 17/111 [11:05<59:46, 38.15s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 18/111 [12:07<1:10:26, 45.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 19/111 [13:14<1:19:28, 51.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 18%|█▊        | 20/111 [13:35<1:04:29, 42.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 19%|█▉        | 21/111 [13:55<53:40, 35.79s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|█▉        | 22/111 [14:31<53:01, 35.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 21%|██        | 23/111 [14:56<47:39, 32.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 24/111 [15:21<43:49, 30.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 25/111 [15:45<41:03, 28.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 26/111 [16:09<38:33, 27.22s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 24%|██▍       | 27/111 [16:35<37:38, 26.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▌       | 28/111 [17:47<55:36, 40.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 29/111 [17:56<42:14, 30.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 27%|██▋       | 30/111 [18:11<35:16, 26.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 31/111 [19:22<52:50, 39.63s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 32/111 [20:33<1:04:38, 49.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|██▉       | 33/111 [21:44<1:12:06, 55.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 34/111 [22:30<1:07:34, 52.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 35/111 [23:41<1:13:43, 58.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 36/111 [24:07<1:00:46, 48.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 33%|███▎      | 37/111 [24:19<46:12, 37.46s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 38/111 [25:30<57:52, 47.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▌      | 39/111 [26:42<1:05:55, 54.94s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 36%|███▌      | 40/111 [27:31<1:02:47, 53.06s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 41/111 [27:39<46:20, 39.72s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 38%|███▊      | 42/111 [28:23<47:01, 40.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 39%|███▊      | 43/111 [29:37<57:50, 51.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|███▉      | 44/111 [30:53<1:05:02, 58.25s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████      | 45/111 [31:09<50:16, 45.71s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 41%|████▏     | 46/111 [32:25<59:27, 54.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 47/111 [33:40<1:04:48, 60.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 48/111 [34:55<1:08:26, 65.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 49/111 [35:04<50:00, 48.39s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▌     | 50/111 [35:13<37:02, 36.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 51/111 [35:54<37:50, 37.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 47%|████▋     | 52/111 [37:10<48:24, 49.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 48%|████▊     | 53/111 [37:54<46:04, 47.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 49%|████▊     | 54/111 [39:12<54:02, 56.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|████▉     | 55/111 [40:27<58:02, 62.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 56/111 [41:47<1:01:51, 67.48s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████▏    | 57/111 [43:02<1:02:46, 69.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 52%|█████▏    | 58/111 [43:18<47:28, 53.75s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 53%|█████▎    | 59/111 [43:29<35:25, 40.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 60/111 [43:36<26:06, 30.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▍    | 61/111 [43:58<23:28, 28.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 56%|█████▌    | 62/111 [45:17<35:25, 43.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for paragraph in tqdm(text):\n",
    "    agent.step(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.humemai.save_db_as_json(\"Llama-3.2-3B-Instruct-8bit.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.humemai.load_db_from_json(\"Llama-3.2-3B-Instruct-8bit.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humemai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
