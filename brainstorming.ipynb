{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gremlinpython:Creating Client with url 'ws://localhost:8282/gremlin'\n",
      "INFO:__main__:Total number of vertices: 68805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vertices: 68805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total number of edges: 53987\n",
      "INFO:gremlinpython:Closing Client with url 'ws://localhost:8282/gremlin'\n",
      "INFO:__main__:Disconnected from the Gremlin server.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Edges: 53987\n"
     ]
    }
   ],
   "source": [
    "# Cell: JanusGraph Manager - Add and Delete Data with Streaming and Corrected Counting\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import logging\n",
    "import random\n",
    "from itertools import islice\n",
    "from gremlin_python.driver.client import Client\n",
    "from gremlin_python.driver.protocol import GremlinServerError\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (useful in Jupyter notebooks)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Gremlin Client\n",
    "GREMLIN_SERVER_URL = \"ws://localhost:8282/gremlin\"  # Update if different\n",
    "client = Client(GREMLIN_SERVER_URL, \"g\")\n",
    "\n",
    "# # Define the 'entity_instance_of' dictionary\n",
    "# # For streaming purposes, consider loading data from an external source or generator\n",
    "# # Here, we'll simulate streaming with a generator function\n",
    "# entity_instance_of = {\n",
    "#     \"Dog\": [\"Mammal\", \"Canine\"],\n",
    "#     \"Cat\": [\"Mammal\", \"Feline\"],\n",
    "#     \"Snake\": [\"Reptile\", \"Serpent\"],\n",
    "#     \"Lizard\": [\"Reptile\"],\n",
    "#     \"Crocodile\": [\"Reptile\", \"Crocodilian\"],\n",
    "#     \"Crocodilian\": [\"Reptile\"],\n",
    "#     \"Mammal\": [\"Animal\"],\n",
    "#     \"Reptile\": [\"Animal\"],\n",
    "#     \"Canine\": [\"Carnivore\"],\n",
    "#     \"Feline\": [\"Carnivore\"],\n",
    "#     \"Carnivore\": [\"Animal\"],\n",
    "#     \"Serpent\": [\"Reptile\"],\n",
    "#     \"Animal\": [],  # Top-level entity with no parents\n",
    "#     # Add more entities and relationships as needed\n",
    "# }\n",
    "\n",
    "\n",
    "def data_generator(entity_dict, slice_size=10000):\n",
    "    \"\"\"\n",
    "    Generator to yield data slices from the entity_instance_of dictionary.\n",
    "\n",
    "    Args:\n",
    "        entity_dict (dict): Dictionary mapping child labels to parent labels.\n",
    "        slice_size (int): Number of entities to process per slice.\n",
    "\n",
    "    Yields:\n",
    "        tuple: (set of unique labels, list of (child_label, parent_label) tuples)\n",
    "    \"\"\"\n",
    "    it = iter(entity_dict.items())\n",
    "    while True:\n",
    "        slice_data = dict(islice(it, slice_size))\n",
    "        if not slice_data:\n",
    "            break\n",
    "        unique_labels = set()\n",
    "        relationships = []\n",
    "        for child_label, parent_labels in slice_data.items():\n",
    "            unique_labels.add(child_label)\n",
    "            for parent_label in parent_labels:\n",
    "                unique_labels.add(parent_label)\n",
    "                relationships.append((child_label, parent_label))\n",
    "        yield unique_labels, relationships\n",
    "\n",
    "\n",
    "async def submit_query(query, operation=\"query\"):\n",
    "    \"\"\"\n",
    "    Submit a Gremlin query with basic error handling.\n",
    "\n",
    "    Args:\n",
    "        query (str): The Gremlin query string.\n",
    "        operation (str): Description of the operation (for logging purposes).\n",
    "\n",
    "    Returns:\n",
    "        ResultSet: The result of the query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        future = client.submitAsync(query)\n",
    "        result = await asyncio.wrap_future(future)\n",
    "        logger.info(f\"Successfully executed {operation}.\")\n",
    "        return result\n",
    "    except GremlinServerError as e:\n",
    "        logger.error(f\"Gremlin Server Error during {operation}: {e.message}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during {operation}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def upsert_vertex(label):\n",
    "    \"\"\"\n",
    "    Insert a vertex with the given label if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        label (str): The unique label of the entity.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        g.V().hasLabel('{label}').fold().\n",
    "        coalesce(\n",
    "            unfold(),\n",
    "            addV('{label}')\n",
    "        )\n",
    "    \"\"\"\n",
    "    await submit_query(query, operation=f\"upserting vertex '{label}'\")\n",
    "\n",
    "\n",
    "async def upsert_edge(child_label, parent_label):\n",
    "    \"\"\"\n",
    "    Insert an edge 'subclass_of' from child to parent if it doesn't already exist.\n",
    "\n",
    "    Args:\n",
    "        child_label (str): Label of the child entity.\n",
    "        parent_label (str): Label of the parent entity.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        g.V().hasLabel('{child_label}').as('child').\n",
    "        V().hasLabel('{parent_label}').as('parent').\n",
    "        coalesce(\n",
    "            __.select('child').outE('subclass_of').where(__.inV().as('parent')),\n",
    "            __.select('child').addE('subclass_of').to('parent')\n",
    "        )\n",
    "    \"\"\"\n",
    "    await submit_query(\n",
    "        query, operation=f\"upserting edge from '{child_label}' to '{parent_label}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def process_vertices(unique_labels, max_workers=3):\n",
    "    \"\"\"\n",
    "    Process and insert all unique vertices with controlled concurrency.\n",
    "\n",
    "    Args:\n",
    "        unique_labels (set): Set of unique entity labels.\n",
    "        max_workers (int): Number of concurrent tasks for processing.\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "\n",
    "    async def sem_upsert_vertex(label):\n",
    "        async with semaphore:\n",
    "            await upsert_vertex(label)\n",
    "\n",
    "    tasks = [asyncio.create_task(sem_upsert_vertex(label)) for label in unique_labels]\n",
    "\n",
    "    # Await all tasks and handle exceptions\n",
    "    for f in tqdm(\n",
    "        asyncio.as_completed(tasks), total=len(tasks), desc=\"Upserting Vertices\"\n",
    "    ):\n",
    "        try:\n",
    "            await f\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during vertex upsert: {e}\")\n",
    "\n",
    "\n",
    "async def process_edges(relationships, batch_size=5, max_workers=3):\n",
    "    \"\"\"\n",
    "    Process and insert all edges in batches with controlled concurrency.\n",
    "\n",
    "    Args:\n",
    "        relationships (list): List of (child, parent) tuples.\n",
    "        batch_size (int): Number of relationships to process per batch.\n",
    "        max_workers (int): Number of concurrent tasks for processing.\n",
    "    \"\"\"\n",
    "    total = len(relationships)\n",
    "    logger.info(f\"Total relationships to process: {total}\")\n",
    "\n",
    "    # Split into batches\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = relationships[i : i + batch_size]\n",
    "        batch_number = i // batch_size + 1\n",
    "        logger.info(\n",
    "            f\"Processing edge batch {batch_number} with {len(batch)} relationships.\"\n",
    "        )\n",
    "        await _process_edge_batch(batch, max_workers)\n",
    "        logger.info(f\"Edge batch {batch_number} processed.\")\n",
    "\n",
    "\n",
    "async def _process_edge_batch(batch, max_workers):\n",
    "    \"\"\"\n",
    "    Process a single batch of edges using limited concurrency.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of (child, parent) tuples.\n",
    "        max_workers (int): Number of concurrent tasks for processing.\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "\n",
    "    async def sem_upsert_edge(child, parent):\n",
    "        async with semaphore:\n",
    "            await upsert_edge(child, parent)\n",
    "\n",
    "    tasks = [\n",
    "        asyncio.create_task(sem_upsert_edge(child, parent)) for child, parent in batch\n",
    "    ]\n",
    "\n",
    "    # Await all tasks and handle exceptions\n",
    "    for f in tqdm(\n",
    "        asyncio.as_completed(tasks), total=len(tasks), desc=\"Upserting Edges\"\n",
    "    ):\n",
    "        try:\n",
    "            await f\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during edge upsert: {e}\")\n",
    "\n",
    "\n",
    "def get_data_stream(entity_dict, slice_size=10000):\n",
    "    \"\"\"\n",
    "    Create a generator to stream data slices from the entity_instance_of dictionary.\n",
    "\n",
    "    Args:\n",
    "        entity_dict (dict): Dictionary mapping child labels to parent labels.\n",
    "        slice_size (int): Number of entities to process per slice.\n",
    "\n",
    "    Yields:\n",
    "        tuple: (set of unique labels, list of (child_label, parent_label) tuples)\n",
    "    \"\"\"\n",
    "    return data_generator(entity_dict, slice_size)\n",
    "\n",
    "\n",
    "async def delete_all_vertices():\n",
    "    \"\"\"\n",
    "    Delete all vertices (and consequently all edges) from the JanusGraph database.\n",
    "    \"\"\"\n",
    "    delete_query = \"g.V().drop().iterate()\"\n",
    "    try:\n",
    "        await submit_query(delete_query, operation=\"deleting all vertices and edges\")\n",
    "        logger.info(\n",
    "            \"All vertices and their associated edges have been successfully deleted.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to delete all data: {e}\")\n",
    "\n",
    "\n",
    "async def delete_vertices_in_batches(batch_size=100):\n",
    "    \"\"\"\n",
    "    Delete all vertices in smaller batches to minimize load and lock contention.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of vertices to delete per batch.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting deletion of vertices in batches...\")\n",
    "    while True:\n",
    "        delete_batch_query = f\"\"\"\n",
    "            g.V().limit({batch_size}).drop().iterate()\n",
    "        \"\"\"\n",
    "        try:\n",
    "            await submit_query(\n",
    "                delete_batch_query,\n",
    "                operation=f\"deleting a batch of {batch_size} vertices\",\n",
    "            )\n",
    "            # Check if any vertices remain\n",
    "            count_query = \"g.V().count()\"\n",
    "            result = await submit_query(\n",
    "                count_query, operation=\"counting remaining vertices\"\n",
    "            )\n",
    "            if result:\n",
    "                counts = result.all().result()\n",
    "                remaining = counts[0] if counts else 0\n",
    "                logger.info(\n",
    "                    f\"Deleted a batch of {batch_size} vertices. Remaining vertices: {remaining}\"\n",
    "                )\n",
    "                if remaining == 0:\n",
    "                    logger.info(\"All vertices have been successfully deleted.\")\n",
    "                    break\n",
    "            else:\n",
    "                logger.warning(\"Failed to retrieve remaining vertex count.\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to delete a batch of vertices: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "async def add_data(\n",
    "    slice_size=10000, max_workers_vertices=3, max_workers_edges=10, batch_size=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Add data to the JanusGraph database by inserting vertices and edges.\n",
    "\n",
    "    Args:\n",
    "        slice_size (int): Number of entities to process per data slice.\n",
    "        max_workers_vertices (int): Number of concurrent tasks for vertex upsertion.\n",
    "        max_workers_edges (int): Number of concurrent tasks for edge upsertion.\n",
    "        batch_size (int): Number of relationships to process per edge batch.\n",
    "    \"\"\"\n",
    "    data_stream = get_data_stream(entity_instance_of, slice_size)\n",
    "    slice_number = 0\n",
    "    for unique_labels, relationships in data_stream:\n",
    "        slice_number += 1\n",
    "        logger.info(\n",
    "            f\"Processing slice {slice_number} with {len(unique_labels)} unique labels and {len(relationships)} relationships.\"\n",
    "        )\n",
    "\n",
    "        # Process all vertices in the current slice\n",
    "        await process_vertices(unique_labels, max_workers=max_workers_vertices)\n",
    "\n",
    "        # Process all edges in the current slice\n",
    "        await process_edges(\n",
    "            relationships, batch_size=batch_size, max_workers=max_workers_edges\n",
    "        )\n",
    "\n",
    "\n",
    "async def delete_data(batch_size=100, method=\"batch\"):\n",
    "    \"\"\"\n",
    "    Delete all data from the JanusGraph database.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of vertices to delete per batch (used in batch deletion).\n",
    "        method (str): Deletion method - 'all' or 'batch'.\n",
    "                      'all' deletes all vertices in one operation.\n",
    "                      'batch' deletes vertices in smaller batches.\n",
    "    \"\"\"\n",
    "    if method == \"all\":\n",
    "        # Option 1: Delete all at once\n",
    "        await delete_all_vertices()\n",
    "    elif method == \"batch\":\n",
    "        # Option 2: Delete in batches to minimize lock contention\n",
    "        await delete_vertices_in_batches(batch_size=batch_size)\n",
    "    else:\n",
    "        logger.error(\"Invalid deletion method specified. Choose 'all' or 'batch'.\")\n",
    "\n",
    "\n",
    "async def close_client_async():\n",
    "    \"\"\"\n",
    "    Close the Gremlin client synchronously.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.close()\n",
    "        logger.info(\"Disconnected from the Gremlin server.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while closing client: {e}\")\n",
    "\n",
    "\n",
    "async def main_async(\n",
    "    operation=\"add\",\n",
    "    slice_size=10000,\n",
    "    max_workers_vertices=3,\n",
    "    max_workers_edges=10,\n",
    "    batch_size=100,\n",
    "    delete_method=\"batch\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Main asynchronous function to perform add or delete operations based on arguments.\n",
    "\n",
    "    Args:\n",
    "        operation (str): 'add' to add data or 'delete' to delete all data.\n",
    "        slice_size (int): Number of entities to process when adding data.\n",
    "        max_workers_vertices (int): Number of concurrent tasks for vertex upsertion.\n",
    "        max_workers_edges (int): Number of concurrent tasks for edge upsertion.\n",
    "        batch_size (int): Number of relationships per edge batch or number of vertices per deletion batch.\n",
    "        delete_method (str): Method of deletion - 'all' or 'batch'.\n",
    "    \"\"\"\n",
    "    if operation == \"add\":\n",
    "        logger.info(\"Starting data addition process...\")\n",
    "        await add_data(\n",
    "            slice_size=slice_size,\n",
    "            max_workers_vertices=max_workers_vertices,\n",
    "            max_workers_edges=max_workers_edges,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    elif operation == \"delete\":\n",
    "        logger.info(\"Starting data deletion process...\")\n",
    "        await delete_data(batch_size=batch_size, method=delete_method)\n",
    "    else:\n",
    "        logger.error(\"Invalid operation specified. Choose 'add' or 'delete'.\")\n",
    "\n",
    "    # Close the Gremlin connection\n",
    "    await close_client_async()\n",
    "\n",
    "\n",
    "def run_operation(\n",
    "    operation=\"add\",\n",
    "    slice_size=10000,\n",
    "    max_workers_vertices=3,\n",
    "    max_workers_edges=10,\n",
    "    batch_size=100,\n",
    "    delete_method=\"batch\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the specified operation asynchronously within the Jupyter notebook.\n",
    "\n",
    "    Args:\n",
    "        operation (str): 'add' to add data or 'delete' to delete all data.\n",
    "        slice_size (int): Number of entities to process when adding data.\n",
    "        max_workers_vertices (int): Number of concurrent tasks for vertex upsertion.\n",
    "        max_workers_edges (int): Number of concurrent tasks for edge upsertion.\n",
    "        batch_size (int): Number of relationships per edge batch or number of vertices per deletion batch.\n",
    "        delete_method (str): Method of deletion - 'all' or 'batch'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        asyncio.run(\n",
    "            main_async(\n",
    "                operation,\n",
    "                slice_size,\n",
    "                max_workers_vertices,\n",
    "                max_workers_edges,\n",
    "                batch_size,\n",
    "                delete_method,\n",
    "            )\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Runtime error: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "async def count_vertices():\n",
    "    \"\"\"\n",
    "    Count the number of vertices in the JanusGraph database.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of vertices.\n",
    "    \"\"\"\n",
    "    query = \"g.V().count()\"\n",
    "    try:\n",
    "        future = client.submitAsync(query)\n",
    "        result = await asyncio.wrap_future(future)\n",
    "        counts = result.all().result()\n",
    "        if counts:\n",
    "            count = counts[0]\n",
    "            logger.info(f\"Total number of vertices: {count}\")\n",
    "            return count\n",
    "        else:\n",
    "            logger.warning(\"No vertices found.\")\n",
    "            return 0\n",
    "    except GremlinServerError as e:\n",
    "        logger.error(f\"Gremlin Server Error: {e.message}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def count_edges():\n",
    "    \"\"\"\n",
    "    Count the number of edges in the JanusGraph database.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of edges.\n",
    "    \"\"\"\n",
    "    query = \"g.E().count()\"\n",
    "    try:\n",
    "        future = client.submitAsync(query)\n",
    "        result = await asyncio.wrap_future(future)\n",
    "        counts = result.all().result()\n",
    "        if counts:\n",
    "            count = counts[0]\n",
    "            logger.info(f\"Total number of edges: {count}\")\n",
    "            return count\n",
    "        else:\n",
    "            logger.warning(\"No edges found.\")\n",
    "            return 0\n",
    "    except GremlinServerError as e:\n",
    "        logger.error(f\"Gremlin Server Error: {e.message}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_count_vertices():\n",
    "    \"\"\"\n",
    "    Synchronously run the asynchronous count_vertices function.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of vertices.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        count = asyncio.run(count_vertices())\n",
    "        return count\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Runtime error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_count_edges():\n",
    "    \"\"\"\n",
    "    Synchronously run the asynchronous count_edges function.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of edges.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        count = asyncio.run(count_edges())\n",
    "        return count\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Runtime error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "# To add data to the JanusGraph database\n",
    "# Adjust the parameters as needed and uncomment the line below to execute\n",
    "\n",
    "# run_operation(\n",
    "#     operation='add',\n",
    "#     slice_size=10000,              # Number of entities per slice\n",
    "#     max_workers_vertices=3,        # Number of concurrent tasks for vertex upsertion\n",
    "#     max_workers_edges=10,          # Number of concurrent tasks for edge upsertion\n",
    "#     batch_size=100,                # Number of relationships per edge batch\n",
    "# )\n",
    "\n",
    "# To delete all data from the JanusGraph database using batch deletion\n",
    "# Adjust the batch_size and method as needed and uncomment the line below to execute\n",
    "\n",
    "# run_operation(\n",
    "#     operation='delete',\n",
    "#     batch_size=100,                # Number of vertices to delete per batch\n",
    "#     delete_method='batch'          # Deletion method: 'all' or 'batch'\n",
    "# )\n",
    "\n",
    "# To delete all data from the JanusGraph database in one go\n",
    "# Uncomment the lines below to execute\n",
    "\n",
    "# run_operation(\n",
    "#     operation='delete',\n",
    "#     batch_size=0,                   # Not used in 'all' method\n",
    "#     delete_method='all'             # Specify deletion method as 'all'\n",
    "# )\n",
    "\n",
    "# To count the number of vertices\n",
    "# Uncomment the lines below to execute\n",
    "\n",
    "total_vertices = run_count_vertices()\n",
    "print(f\"Total Vertices: {total_vertices}\")\n",
    "\n",
    "# To count the number of edges\n",
    "# Uncomment the lines below to execute\n",
    "\n",
    "total_edges = run_count_edges()\n",
    "print(f\"Total Edges: {total_edges}\")\n",
    "\n",
    "# Close the client after all operations are complete\n",
    "try:\n",
    "    client.close()\n",
    "    logger.info(\"Disconnected from the Gremlin server.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error while closing client: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "humemai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
